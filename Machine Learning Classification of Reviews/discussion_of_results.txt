
2.5 Discussion of Results.

Firstly, it's worth mentioning that the classifier has been a success, by this I mean that the lowest averaged accuracy observed seems to be around the 82.9% mark (for cafe reviews) using k-fold cross validation. This was used to reduce the variation within the accuracy due to the randomness of what reviews are allocated to the training set and the testing set, that is that some reviews are 'better' for training a classifier and some are 'worse', so with the creation of each model a different accuracy is generated. The standard deviation was also added to quantify this variation and was found to be 1.9% in the largest case (gym reviews).

K-fold cross validation approach works by dividing the data into 'k' amount of subsets, where each is used as the test set once, creating an accuracy measurement for each model (the number defined to be 5 in this code) and ultimately, an average accuracy over all models. This was also used to produce a more accurate standard deviation value. Accuracy and standard deviation values can be seen in section 2.1.

Looking at variables within the code, it becomes apparent that there are other sources of accuracy variation, two of these are the minimum document frquency of a word and the fraction of data allocated to testing. Functions have been made to investigate the effect of these variables on the overall accuracy of the classification model and although the accuracy of these predictions are still open to the initial variation from what reviews are randomly selected as the test data, the averaging used in the graphing functions minimises this affect and shows the general trend.

The minimum document frequency (min_df) of a word, is how many documents that word must be present in to allow it into the dictionary of words that the classifier searches for. When this is varied and the accuracy averaged each time, we get a graph as seen in section 2.3 (a). This graph shows indication that too small a min_df creates too much 'word noise' to shuffle through, that is the classifier doesn't have a specific enough dictionary of words to differentiate between positive and negative reviews. At the same time, too high a min_df will filter words that would be useful for classification and therefore a balance needs to be met, the graph suggests a ball park value of around 10-20 for this case, although in general this will be proportional to the overall volume of documents being classified.
The fraction of data allocated to testing by proxy also defines the fraction of data allocated to training the classifier. Varying this factor and plotting against averaged accuracies, the graph seen in section 2.3 (b), is produced. As expected, the lower the fraction of data allocated to testing is, the better the accuracy, this is because the classifier has more to learn from but also because it has less room for error when tested on a smaller test sample. This introduces a trade off, where ideally both the testing and training set will be arbitrarily large (known as overfitting), however this is impractical, so for the purpose of this assignment the fraction of testing data was maintained at 0.3.
In terms of classification reports, the values for each category can be seen in section 2.1. Looking at the values it is apparent the most accurate classification model for its category is that modelled on gym reviews, this is assumed to be due to the limitations of the gym experience - where a customer is mostly responsible for their own session, relative to cafes/restaraunts which have more of a review dependence on employee-customer relations. Due to this, I believe that the gym experience is more black and white i.e. you either enjoyed the experience or didn't, while cafes and restaraunts are more ambiguous, where you may enjoy an aspect of the experience and not another. This ambiguity affects the spread of the star rating associated with each review category and this has been displayed in section 2.4, where 75.9% of gym reviews are 1 or 5 star ratings (also having the lowest amount 3 star reviews), contrasted to cafe reviews (60%) and restaraunt reviews (57%). It is reasonable that 1 and 5 star reviews will generally be the easiest to distinguish in terms of being positive or negative, while 3 star ratings are a bit more down the middle and therefore harder for classifiers to label accurately.

Continuing from this point, this highlights why in all three classification models false positives are largely more frequent than false negatives (seen in section 2.2 graphs). This is partly due to the most ambigous rating, '3 stars', being identified to the negative category of labels, although, these 3 star reviews may contain large sections of positive text or they may even be considered subjectively positive by the author. This is also independently backed up by having printed and observed a number of falsely predicted reviews.
It is expected that this is the main factor for false predictions, as false negatives appear to almost be negligable (visualised in the confusion matrices of section 2.2).

In terms of recall and precision, these are terms for measuring a classifiers ability to predict labels free of false positives and false negatives and are best represented by the F1 score which is a weighted average of both of them. The F1 score across the board backs up the fact that negative reviews are often being classified as positive, as suggest due to the amiguity of 3 star rating and its placement in the negative category.


  
3.2 Discussion of Results.

The accuracies in this section are absolute, that is there is no variation each time the code is run, this is because the whole set of data from one category is used to train the model and essentially all the data from the other categories are used for testing (Note: 99.9% of the test categories were used as errors arose for a full 100%). The accuracy of each classification model is summarised as follows:

Cafe Model: 84% Gym, 79% Restaraunt.
Gym Model: 86% Cafe, 82% Restaraunt.
Restaraunt Model: 88% Cafe, 85% Gym.
Firstly, it is apparent that all models work relatively well on all categories, this can be attributed to the commonalities between data sets e.g. they are all in english, they share similiar 'review' related words, they are all either positive or negative in nature and in some cases they may overlap in similar vocabulary (food, customer relations etc.).

Amusingly, some models work better on other categories than they did on their own (seen in section 2.1). This may be an effect of nullifying 'noise' words from the models dictionary, those being words that frequently pop up and are unique to one category, essentially tricking the classifier into relating those words to positive or negative reviews, when in reality these words are neutral and hold no weight on the outlook of a review. Therefore, when the classifier acts on another category's data where these 'noise' words from one category are not present it can only go on genuinely positive or negative review words that hold true over differing categories. It is noted however that this is only a hypothesis, but it leads to an interesting thought, that if a classifier was modelled across all three categories, would it be able to narrow down on the essence of what makes a review positive or negative, free from categorical jargon that may muddy its judgement?

Another possible scenario is that the min_df discussed earlier was simply set too low (apparent from graph 'a' in 2.3), as it seems reasonable to imagine that a a model built on one category will be better at classifying reviews from that one category and as an extension, similar categories. For example this is seen with the restaraunt modelled classfier, it classifies cafes (88%) better than it does gyms (85%) and this makes sense as cafes share more similarities with restaraunts. Although, the cafe model has a higher accuracy for gyms (84%) than it does restaraunts (79%), but the accuracy for both cases isn't too dissimilar so the small difference may be due to a range of factors. For example, it appears in the classification report that the cafe model is quite bad (54%) at classifying negative restaraunt reviews, this may suggest that some negative aspects of a cafe may be positive in a restaraunt setting, such as the behaviour of the staff (professionality, cordiality, etc.). Furthermore, this may just be indicative that there is more in common between cafe and gym reviews than one would be inclined to believe off face value, a commonality that the classifier can pick up on due to the large data sets that it is privy to. To really gain a granular understanding on the root of these discrepancies it may be necessary to write a code to incrementally inspect each false prediction and perhaps classify them in some way, either way this is beyond the scope of this assignment although it may prove an interesting route of investigation for myself moving forward.
